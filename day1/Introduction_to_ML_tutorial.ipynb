{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dba511",
   "metadata": {},
   "source": [
    "# Tutorial for Introduction to ML Lecture\n",
    "\n",
    "version 0.1, September 2023\n",
    "\n",
    "Bryan Scott, CIERA/Northwestern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c6b0f",
   "metadata": {},
   "source": [
    "## Problem 1: Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07842041",
   "metadata": {},
   "source": [
    "A good starting point for Machine Learning is the Bayes classifier. The basic idea is to assign the most probable label to each data point using Bayes theorem, we take:\n",
    "\n",
    "$$\n",
    "p(y | x_n) \\propto p(y)p(x_i, ..., x_n | y)\n",
    "$$\n",
    "\n",
    "where y is a label for a data point and the $x_n$ are the features of the data that we want to use to classify each data point. A $\\textit{Naive} Bayes$ classifier makes an important simplifying assumptions that gives it the name - it assumes that the conditional probabilities are independent, $p(x_i, ..., x_n | y) = p(x_i|y)... p(x_n | y)$. That is, the probability of observing any individual feature doesn't depend on any of the other features. Our task is to construct this classifier from a set of examples we've observed previously and compare it to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084db8",
   "metadata": {},
   "source": [
    "### Part 0: Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e543f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsst_data[0:1000].to_csv('session_19_DC2_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682ccc",
   "metadata": {},
   "source": [
    "### Loading and splitting the data. \n",
    "\n",
    "Read in the data, then start by selecting the id, fluxes, and object truth type in the lsst data file you've been provided. \n",
    "\n",
    "Once you have selected those, randomly split the data into two arrays, one containing 80% of the data, and a second array containing 20% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94094b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_data = pd.read_csv('data/table_data.lsst.cloudapi.csv')\n",
    "\n",
    "# solved \n",
    "\n",
    "lsst_data_to_classify = lsst_data[['id', 'flux_g', 'flux_i', 'flux_r', 'flux_u', 'flux_y', 'flux_z', 'truth_type']]\n",
    "random_data = lsst_data_to_classify.sample(n=1000)\n",
    "\n",
    "random_data.to_csv('simulated_extragalactic_data.csv')\n",
    "\n",
    "train_data = random_data.iloc[0:800]\n",
    "test_data = random_data.iloc[800:1000]\n",
    "\n",
    "# unsolved \n",
    "\n",
    "# lsst_data_to_classify = \n",
    "# random_data = \n",
    "\n",
    "# train_data = \n",
    "# test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ff799b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flux_g</th>\n",
       "      <th>flux_i</th>\n",
       "      <th>flux_r</th>\n",
       "      <th>flux_u</th>\n",
       "      <th>flux_y</th>\n",
       "      <th>flux_z</th>\n",
       "      <th>truth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48344</th>\n",
       "      <td>11568460784</td>\n",
       "      <td>10.56370</td>\n",
       "      <td>29.4790</td>\n",
       "      <td>16.4745</td>\n",
       "      <td>3.79227</td>\n",
       "      <td>76.0767</td>\n",
       "      <td>45.1033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17666</th>\n",
       "      <td>11626701310</td>\n",
       "      <td>151.17900</td>\n",
       "      <td>267.0960</td>\n",
       "      <td>157.2470</td>\n",
       "      <td>159.34900</td>\n",
       "      <td>287.3300</td>\n",
       "      <td>292.1570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47807</th>\n",
       "      <td>11566532886</td>\n",
       "      <td>19.39760</td>\n",
       "      <td>30.3091</td>\n",
       "      <td>24.4655</td>\n",
       "      <td>19.74380</td>\n",
       "      <td>72.5930</td>\n",
       "      <td>47.4641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>11564209917</td>\n",
       "      <td>279.80400</td>\n",
       "      <td>1925.0200</td>\n",
       "      <td>754.7330</td>\n",
       "      <td>185.44200</td>\n",
       "      <td>3909.2000</td>\n",
       "      <td>3307.4300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37532</th>\n",
       "      <td>11688356116</td>\n",
       "      <td>5.14646</td>\n",
       "      <td>18.9025</td>\n",
       "      <td>11.6335</td>\n",
       "      <td>3.25081</td>\n",
       "      <td>21.5846</td>\n",
       "      <td>20.3508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49702</th>\n",
       "      <td>11569887601</td>\n",
       "      <td>11.08150</td>\n",
       "      <td>30.9775</td>\n",
       "      <td>18.1020</td>\n",
       "      <td>2.29094</td>\n",
       "      <td>72.4049</td>\n",
       "      <td>52.0237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14466</th>\n",
       "      <td>11562553858</td>\n",
       "      <td>70.30830</td>\n",
       "      <td>304.1930</td>\n",
       "      <td>200.4590</td>\n",
       "      <td>16.73550</td>\n",
       "      <td>454.9000</td>\n",
       "      <td>385.6030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23077</th>\n",
       "      <td>11638125364</td>\n",
       "      <td>86.73320</td>\n",
       "      <td>77.4096</td>\n",
       "      <td>83.8354</td>\n",
       "      <td>69.89100</td>\n",
       "      <td>77.4372</td>\n",
       "      <td>77.5890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42190</th>\n",
       "      <td>11691538627</td>\n",
       "      <td>5.42994</td>\n",
       "      <td>24.6955</td>\n",
       "      <td>11.7521</td>\n",
       "      <td>2.15128</td>\n",
       "      <td>87.6585</td>\n",
       "      <td>49.3406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17384</th>\n",
       "      <td>11570995713</td>\n",
       "      <td>42.92020</td>\n",
       "      <td>36.9235</td>\n",
       "      <td>40.1047</td>\n",
       "      <td>40.34520</td>\n",
       "      <td>33.9244</td>\n",
       "      <td>35.7940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     flux_g     flux_i    flux_r     flux_u     flux_y  \\\n",
       "48344  11568460784   10.56370    29.4790   16.4745    3.79227    76.0767   \n",
       "17666  11626701310  151.17900   267.0960  157.2470  159.34900   287.3300   \n",
       "47807  11566532886   19.39760    30.3091   24.4655   19.74380    72.5930   \n",
       "8352   11564209917  279.80400  1925.0200  754.7330  185.44200  3909.2000   \n",
       "37532  11688356116    5.14646    18.9025   11.6335    3.25081    21.5846   \n",
       "...            ...        ...        ...       ...        ...        ...   \n",
       "49702  11569887601   11.08150    30.9775   18.1020    2.29094    72.4049   \n",
       "14466  11562553858   70.30830   304.1930  200.4590   16.73550   454.9000   \n",
       "23077  11638125364   86.73320    77.4096   83.8354   69.89100    77.4372   \n",
       "42190  11691538627    5.42994    24.6955   11.7521    2.15128    87.6585   \n",
       "17384  11570995713   42.92020    36.9235   40.1047   40.34520    33.9244   \n",
       "\n",
       "          flux_z  truth_type  \n",
       "48344    45.1033           1  \n",
       "17666   292.1570           1  \n",
       "47807    47.4641           1  \n",
       "8352   3307.4300           1  \n",
       "37532    20.3508           1  \n",
       "...          ...         ...  \n",
       "49702    52.0237           1  \n",
       "14466   385.6030           1  \n",
       "23077    77.5890           1  \n",
       "42190    49.3406           1  \n",
       "17384    35.7940           1  \n",
       "\n",
       "[848 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data[random_data['truth_type'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cb839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99f2de21",
   "metadata": {},
   "source": [
    "### Part 1: Estimate Class Frequency in the training set\n",
    "\n",
    "One of the ingredients in our classifier is p(y), the unconditional class probabilities. \n",
    "\n",
    "We can get this by counting the number of rows belonging to each class in train_data and dividing by the length of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0186b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved\n",
    "\n",
    "def estimate_class_probabilities(x_train): \n",
    "    \"\"\"\n",
    "    Computes unconditional class probabilities. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        ints p1, p2: unconditional probability of an element of the training set belonging to class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = len(x_train.loc[x_train['truth_type'] == 1])/800\n",
    "    p2 = len(x_train.loc[x_train['truth_type'] == 2])/800\n",
    "    \n",
    "    return p1, p2\n",
    "\n",
    "p1, p2 = estimate_class_probabilities(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa268",
   "metadata": {},
   "source": [
    "### Part 2:  Feature Likelihoods\n",
    "\n",
    "We are assuming that the relationship between the classes and feature probabilities are related via:\n",
    "\n",
    "$p(x_i, ..., x_n | y) =  p(x_i|y)... p(x_n | y)$\n",
    "\n",
    "however, we still need to make an assumption about the functional form of the $p(x_n | y)$. As a simple case, we will assume p(x_n | y) follows a Gaussian distribution given by:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp{\\left(-\\frac{(x_i - \\mu)^2}{\\sigma_y^2}\\right)}\n",
    "$$\n",
    "\n",
    "and we will make a maximum likelihood estimate of $\\mu$ and $\\sigma_y$ from the data. This means using empirical estimates $\\bar{x}$ and $\\hat{\\sigma}$ as estimators of the true parameters $\\mu$ and $\\sigma_y$. \n",
    "\n",
    "Write a fitting function that takes the log of the fluxes and returns an estimate of the parameters of the per-feature likelihoods for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "609b65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved\n",
    "\n",
    "def per_feature_likelihood_parameters(x_train, label):\n",
    "    \"\"\"\n",
    "    Computes MAP estimates for the class conditional likelihood. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    "        label (int): training labels for the classifier \n",
    " \n",
    "    Returns:\n",
    "        means, stdevs (array): MAP estimates of the Gaussian conditional probability distributions for a specific class\n",
    "    \"\"\"\n",
    "    \n",
    "    means = np.log(x_train.loc[x_train['truth_type'] == label][['flux_g', 'flux_r', 'flux_i', 'flux_u', 'flux_y']]).mean()\n",
    "    stdevs = np.log(x_train.loc[x_train['truth_type'] == label][['flux_g', 'flux_r', 'flux_i', 'flux_u', 'flux_y']]).std()\n",
    "    \n",
    "    return means, stdevs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbeb61",
   "metadata": {},
   "source": [
    "### Part 3: MAP Estimates of the Class Probabilities\n",
    "\n",
    "Now that we have the unconditional class probabilities and the parameters of the per feature likelihoods in hand, we can put this all together to build the classifier. Use the methods you have already written to write a function that takes in the training data and returns fit parameters. Once you have done that, write a method that takes the fit parameters as an argument and predicts the class of new (and unseen) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05a17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "\n",
    "# solved \n",
    "\n",
    "def fit(x_train):\n",
    "    \"\"\"\n",
    "    Convenience function to perform fitting on the training data\n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std: see documentation for per_feature_likelihood_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    p1, p2 = estimate_class_probabilities(x_train)\n",
    "    \n",
    "    class_1_mean, class_1_std = per_feature_likelihood_parameters(x_train, 1)\n",
    "    class_2_mean, class_2_std = per_feature_likelihood_parameters(x_train, 2)\n",
    "    \n",
    "    return p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bcad9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probability, class_means, class_dev = fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01867cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved \n",
    "\n",
    "def predict(x_test, class_probability, class_means, class_dev):\n",
    "    \"\"\"\n",
    "    Predict method\n",
    "     \n",
    "    Args:\n",
    "        x_test (array): data to perform classification on\n",
    "        class_probability (array): unconditional class probabilities\n",
    "        class_means, class_dev (array): MAP estimates produced by the fit method\n",
    " \n",
    "    Returns:\n",
    "        predict_List (list): class membership predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    flux_g, flux_r, flux_i, flux_u, flux_y = np.log(np.array(x_test[['flux_g', 'flux_r', 'flux_i', 'flux_u', 'flux_y']])).T\n",
    "\n",
    "    prob_1 = class_probability[0] * norm.pdf(flux_i, class_means[0]['flux_i'],class_dev[0]['flux_i']) * norm.pdf(flux_r, class_means[0]['flux_r'],class_dev[0]['flux_r'])* norm.pdf(flux_g, class_means[0]['flux_g'],class_dev[0]['flux_g'])* norm.pdf(flux_u, class_means[0]['flux_u'],class_dev[0]['flux_u'])* norm.pdf(flux_y, class_means[0]['flux_y'],class_dev[0]['flux_y'])\n",
    "    prob_2 = class_probability[1] * norm.pdf(flux_i, class_means[1]['flux_i'],class_dev[1]['flux_i'])* norm.pdf(flux_r, class_means[1]['flux_r'],class_dev[1]['flux_r']) * norm.pdf(flux_g, class_means[1]['flux_g'],class_dev[1]['flux_g']) * norm.pdf(flux_u, class_means[1]['flux_u'],class_dev[0]['flux_u'])* norm.pdf(flux_y, class_means[1]['flux_y'],class_dev[1]['flux_y'])\n",
    "    \n",
    "    predict_list = np.zeros(len(prob_1))\n",
    "        \n",
    "    for i in range(0, len(prob_1)):\n",
    "        if prob_1[i] >= prob_2[i]:\n",
    "            predict_list[i] = int(1)\n",
    "            \n",
    "        if prob_1[i] < prob_2[i]:\n",
    "            predict_list[i] = int(2)\n",
    "    \n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ece7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[~(test_data == 0).any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7865e066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 2., 1., 2., 1., 1., 2., 2., 1., 1., 2., 2.,\n",
       "       2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 2., 2., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2.,\n",
       "       2., 2., 1., 1., 1., 1., 1., 2., 1., 1., 1., 2., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_data, class_probability, class_means, class_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067d9e3",
   "metadata": {},
   "source": [
    "### Part 4: Metrics\n",
    "\n",
    "After creating a classifier, you now want to evaluate it in terms of how often it correctly and incorrectly classifies the objects in your training set. To do this, we'll design a confusion matrix. A confusion matrix is a matrix whose entries are the counts of the predicted vs actual class. For example, the first entry is the count of objects that are predicted to be of class 1 and actually are of class 1 and so on, while the off-diagonal elements would be instances of class 1 that are predicted to be of class 2, and instances of class 2 that are predicted to be of class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a366c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a50e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.crosstab(test_data['truth_type'] ,predict(test_data, class_probability, class_means, class_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fde1da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c8763",
   "metadata": {},
   "source": [
    "## Problem 2: The Cramer-Rao bound (pen & paper, challenging)\n",
    "\n",
    "As we saw in the lecture, the Cramer-Rao bound is an important result in statistics that has intuitive consequences for many applied problems in ML. The proof of the Cramer-Rao bound can be insightful to work through. \n",
    "\n",
    "The starting point for the proof of the bound is the Cauchy-Schwarz inequality, which can be used to show that:\n",
    "\n",
    "$$\n",
    "[Cov(U, V)]^2 \\le Var(U)Var(V)\n",
    "$$\n",
    "\n",
    "Starting from the definitions that U = T(X), where T(X) is an estimator of some parameter $\\theta$ of the distribution $f(X|\\theta)$ from which the data is sampled, and V = $\\frac{\\partial}{\\partial \\theta} log f(X |\\theta)$. Use the Cauchy-Schwarz inequality to show the Cramer-Rao bound for these choices of U and V. \n",
    "\n",
    "$\\textit{Hint:}$ you will need the fact that the $\\mathbb{E}(V) = 0$, where $\\mathbb{E}$ is the expectation of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1432f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
